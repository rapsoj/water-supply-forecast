{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe0831a",
   "metadata": {},
   "source": [
    "# Process GRACE Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea7537",
   "metadata": {},
   "source": [
    "### Prepare Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas xarray netCDF4\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "# Define the base directory and the subfolder names\n",
    "base_dir = '/Users/jessicarapson/Documents/GitHub/water-supply-forecast' # Change this\n",
    "nc_files_location = 'assets/data/grace_indicators'\n",
    "gpkg_file_path = 'assets/data/geospatial.gpkg'\n",
    "\n",
    "# Set the current working directory to the base directory\n",
    "os.chdir(base_dir)\n",
    "\n",
    "# Create the full path to the subfolders\n",
    "nc_files_folder = os.path.join(base_dir, nc_files_location)\n",
    "\n",
    "# Load GeoPackage file containing polygons\n",
    "gdf = gpd.read_file(gpkg_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f27302",
   "metadata": {},
   "source": [
    "### Export Catchment Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3eb68465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the derived data\n",
    "dfs = []\n",
    "\n",
    "# List all folders (years) in the subfolder\n",
    "nc_files_folder = [f.path for f in os.scandir(nc_files_location) if f.is_dir()]\n",
    "\n",
    "# Loop through each year's folder\n",
    "for year_folder in nc_files_folder:\n",
    "    \n",
    "    # List all .nc4 files in the current year's folder\n",
    "    nc_files = [f.path for f in os.scandir(year_folder) if f.is_file() and f.name.endswith('.nc4')]\n",
    "    \n",
    "    # Loop through each .nc4 file in the year's folder\n",
    "    for nc_file in nc_files:\n",
    "        \n",
    "        # Open the NetCDF file using xarray\n",
    "        data = xr.open_dataset(nc_file)\n",
    "        \n",
    "        # Loop through the catchments\n",
    "        for index, row in gdf.iterrows():\n",
    "            \n",
    "            # Select catchment area\n",
    "            polygon = row['geometry']\n",
    "            \n",
    "            # Select only raster data that overlaps with the catchment\n",
    "            data_subset = data.sel(lat=slice(polygon.bounds[1], polygon.bounds[3]),\n",
    "                                   lon=slice(polygon.bounds[0], polygon.bounds[2]))\n",
    "            \n",
    "            # Record the site and time\n",
    "            site_id = gdf.iloc[index]['site_id']\n",
    "            time =  pd.to_datetime(data_subset.coords['time'].values.item())\n",
    "\n",
    "            # Take the cachment mean for each value\n",
    "            gws_inst = np.nanmean(data_subset['gws_inst'].values)\n",
    "            rtzsm_inst = np.nanmean(data_subset['rtzsm_inst'].values)\n",
    "            sfsm_inst = np.nanmean(data_subset['sfsm_inst'].values)\n",
    "            \n",
    "            # Create a dataframe with the selected data\n",
    "            subset_df = pd.DataFrame({\n",
    "                'site_id': site_id,\n",
    "                'time': time,\n",
    "                'mean_gws_inst': gws_inst,\n",
    "                'mean_rtzsm_inst': rtzsm_inst,\n",
    "                'mean_sfsm_inst': sfsm_inst}, index=[0])\n",
    "        \n",
    "            # Append the subset dataframe to the output dataframe\n",
    "            dfs.append(subset_df)\n",
    "        \n",
    "        # Once done, close the dataset\n",
    "        data.close()\n",
    "        \n",
    "# Concatenate all dataframes into a single dataframe\n",
    "result_df = pd.DataFrame(pd.concat(dfs, ignore_index=True))\n",
    "\n",
    "# Export the dataframe\n",
    "result_df.to_csv(\"assets/data/grace_indicators/grace_aggregated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b509d363",
   "metadata": {},
   "source": [
    "### Export Grid Values for Each Catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "daac93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the derived data\n",
    "dfs = []\n",
    "\n",
    "# List all folders (years) in the subfolder\n",
    "nc_files_folder = [f.path for f in os.scandir(nc_files_location) if f.is_dir()]\n",
    "        \n",
    "# Loop through the catchments\n",
    "for index, row in gdf.iterrows():\n",
    "    \n",
    "    # Select catchment area\n",
    "    polygon = row['geometry']\n",
    "\n",
    "    # Loop through each year's folder\n",
    "    for year_folder in nc_files_folder:\n",
    "        \n",
    "        # List all .nc4 files in the current year's folder\n",
    "        nc_files = [f.path for f in os.scandir(year_folder) if f.is_file() and f.name.endswith('.nc4')]\n",
    "    \n",
    "        # Loop through each .nc4 file in the year's folder\n",
    "        for nc_file in nc_files:\n",
    "\n",
    "            # Select only raster data that overlaps with the catchment\n",
    "            data_subset = data.sel(lat=slice(polygon.bounds[1], polygon.bounds[3]),\n",
    "                                   lon=slice(polygon.bounds[0], polygon.bounds[2]))\n",
    "\n",
    "            # Record the site and time\n",
    "            site_id = gdf.iloc[index]['site_id']\n",
    "            time =  pd.to_datetime(data_subset.coords['time'].values.item())\n",
    "\n",
    "            # Format lat and long coordinates\n",
    "            lat = data_subset.coords['lat'].values\n",
    "            lon = data_subset.coords['lon'].values\n",
    "            lat_mesh, lon_mesh = np.meshgrid(lat, lon)\n",
    "            lat_mesh = lat_mesh.flatten()\n",
    "            lon_mesh = lon_mesh.flatten()\n",
    "\n",
    "            # Select values\n",
    "            values = data_subset['gws_inst'].values.flatten()\n",
    "            \n",
    "            # Create a dataframe with the selected data\n",
    "            subset_df = pd.DataFrame({\n",
    "            'site_id': site_id,\n",
    "            'time': time,\n",
    "            'lat': lat_mesh,\n",
    "            'lon': lon_mesh,\n",
    "            'gws_inst': values})\n",
    "            \n",
    "            # Append the subset dataframe to the output dataframe\n",
    "            dfs.append(subset_df)\n",
    "            \n",
    "            # Once done, close the dataset\n",
    "            data.close()\n",
    "        \n",
    "# Concatenate all dataframes into a single dataframe\n",
    "result_df = pd.DataFrame(pd.concat(dfs, ignore_index=True))\n",
    "\n",
    "# Export the dataframe\n",
    "result_df.to_csv(\"assets/data/grace_indicators/grace_pixels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ec3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
