{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b214e4a",
   "metadata": {},
   "source": [
    "# Applied Climate Information System (ACIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9c0cf",
   "metadata": {},
   "source": [
    "### Prepare Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6809904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import system libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Import geospatial libraries\n",
    "import geopandas as gpd\n",
    "\n",
    "# Import API libraries\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/Users/jessicarapson/Documents/GitHub/water-supply-forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416116a9",
   "metadata": {},
   "source": [
    "### Load Data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3259823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ACIS for: hungry_horse_reservoir_inflow (1/26)\n",
      "Processing ACIS for: snake_r_nr_heise (2/26)\n",
      "Processing ACIS for: pueblo_reservoir_inflow (3/26)\n",
      "Processing ACIS for: sweetwater_r_nr_alcova (4/26)\n",
      "Processing ACIS for: missouri_r_at_toston (5/26)\n",
      "Processing ACIS for: animas_r_at_durango (6/26)\n",
      "Processing ACIS for: yampa_r_nr_maybell (7/26)\n",
      "Processing ACIS for: libby_reservoir_inflow (8/26)\n",
      "Processing ACIS for: boise_r_nr_boise (9/26)\n",
      "Processing ACIS for: green_r_bl_howard_a_hanson_dam (10/26)\n",
      "Processing ACIS for: taylor_park_reservoir_inflow (11/26)\n",
      "Processing ACIS for: dillon_reservoir_inflow (12/26)\n",
      "Processing ACIS for: ruedi_reservoir_inflow (13/26)\n",
      "Processing ACIS for: fontenelle_reservoir_inflow (14/26)\n",
      "Processing ACIS for: weber_r_nr_oakley (15/26)\n",
      "Processing ACIS for: san_joaquin_river_millerton_reservoir (16/26)\n",
      "Processing ACIS for: merced_river_yosemite_at_pohono_bridge (17/26)\n",
      "Processing ACIS for: american_river_folsom_lake (18/26)\n",
      "Processing ACIS for: colville_r_at_kettle_falls (19/26)\n",
      "Processing ACIS for: stehekin_r_at_stehekin (20/26)\n",
      "Processing ACIS for: detroit_lake_inflow (21/26)\n",
      "Processing ACIS for: virgin_r_at_virtin (22/26)\n",
      "Processing ACIS for: skagit_ross_reservoir (23/26)\n",
      "Processing ACIS for: boysen_reservoir_inflow (24/26)\n",
      "Processing ACIS for: pecos_r_nr_pecos (25/26)\n",
      "Processing ACIS for: owyhee_r_bl_owyhee_dam (26/26)\n"
     ]
    }
   ],
   "source": [
    "# Load in site geospatial data\n",
    "gdf_sites = gpd.read_file('assets/data/geospatial.gpkg')\n",
    "\n",
    "# Initialize an empty list to store catchment bounding boxes\n",
    "site_bboxes = []\n",
    "\n",
    "# Iterate through each polygon (catchment) in the GeoDataFrame\n",
    "for index, row in gdf_sites.iterrows():\n",
    "    # Get the bounding box for each polygon\n",
    "    bbox = row.geometry.bounds  # Extract the bounding box as (minx, miny, maxx, maxy)\n",
    "    site_bboxes.append(bbox)  # Append the bounding box to the list\n",
    "\n",
    "# Initialise list\n",
    "df_all = []\n",
    "\n",
    "# Loop through catchments\n",
    "for i in range(0,len(gdf_sites)):\n",
    "    print(\"Processing ACIS for:\", gdf_sites.iloc[i]['site_id'], f\"({i + 1}/{len(gdf_sites)})\")\n",
    "\n",
    "    # Call data using web servies\n",
    "    input_dict = {\n",
    "      'bbox': site_bboxes[i],\n",
    "      'sdate' : '1985-01-01',\n",
    "      'edate': '2024-01-01',\n",
    "      'meta' : 'name, sids',\n",
    "      'elems' : [{\n",
    "        'name' : 'pcpn',\n",
    "        'interval' : [0,0,1],\n",
    "        'duration' : \"dly\",\n",
    "        'reduce' : {'reduce':'sum'},\n",
    "      },{\n",
    "        'name' : 'pcpn',\n",
    "        'interval' : [0,0,1],\n",
    "        'duration' : \"dly\",\n",
    "        'reduce' : {'reduce':'sum'},\n",
    "        'normal' : 'departure'\n",
    "      },{\n",
    "        'name' : 'avgt',\n",
    "        'interval' : [0,0,1],\n",
    "        'duration' : \"dly\",\n",
    "        'reduce' : {'reduce':'mean'},\n",
    "      },{\n",
    "        'name' : 'avgt',\n",
    "        'interval' : [0,0,1],\n",
    "        'duration' : \"dly\",\n",
    "        'reduce' : {'reduce':'mean'},\n",
    "        'normal' : 'departure'\n",
    "      }]\n",
    "    }\n",
    "    params = {'params': json.dumps(input_dict)}\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    req = requests.post('http://data.rcc-acis.org/MultiStnData', data=params, headers=headers)\n",
    "    response = req.json()\n",
    "    acis_data = response['data']\n",
    "\n",
    "    # Extract column names\n",
    "    columns = list(acis_data[0]['meta'].keys()) + [f'data_{i+1}' for i in range(\n",
    "        len(acis_data[0]['data'][0]))]\n",
    "\n",
    "    # Extract data values\n",
    "    values = []\n",
    "    for row in acis_data:\n",
    "        meta_values = list(row['meta'].values())\n",
    "        data_values = row['data']\n",
    "        for d in data_values:\n",
    "            values.append(meta_values + d)\n",
    "\n",
    "    # # Create DataFrame\n",
    "    df = pd.DataFrame(values, columns=columns)\n",
    "    df.columns = ['station_name', 'pcpn', 'pcpn_d', 'avgt', 'avgt_d']\n",
    "\n",
    "    # # Create list of weeks\n",
    "    week_list = []\n",
    "\n",
    "    # Define start and end dates\n",
    "    start_date = datetime.date(1985, 1, 1)\n",
    "    end_date = datetime.date(2024, 1, 1)\n",
    "\n",
    "    # Initialize an empty list to store weeks\n",
    "    week_list = []\n",
    "\n",
    "    # Generate weeks between start_date and end_date\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        for day in [1, 8, 15, 22]:\n",
    "            week = current_date + datetime.timedelta(days=(day - current_date.weekday() - 1))\n",
    "            if week < end_date:\n",
    "                week_list.append(week.strftime('%Y-%m-%d'))\n",
    "        current_date += datetime.timedelta(days=7)\n",
    "\n",
    "    # Generate a sequence of dates starting from '1985-01-01'\n",
    "    ndays = len(df[df['station_name'] == df['station_name'][0]])\n",
    "    start_date = datetime.date(1985, 1, 1)\n",
    "    date_sequence = [start_date + datetime.timedelta(days=i) for i in range(ndays)]\n",
    "\n",
    "    # Add the date_sequence as a new column 'Date_Column' in the DataFrame\n",
    "    df['date'] = date_sequence * int(len(df) / ndays)\n",
    "\n",
    "    # Function to round down the day to the nearest value less than or equal to the day\n",
    "    def round_day_down(date):\n",
    "        day = date.day\n",
    "        nearest_values = [1, 8, 15, 22]\n",
    "\n",
    "        # Find the nearest value less than or equal to the day\n",
    "        rounded_day = max(filter(lambda x: x <= day, nearest_values))\n",
    "        return date.replace(day=rounded_day)\n",
    "\n",
    "    # Create a new column 'Rounded_Day_Column' based on 'Date_Column'\n",
    "    df['week_start_date'] = df['date'].apply(round_day_down)\n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    # # Clean data and take average\n",
    "    df = df.replace(['M','T'], np.nan)\n",
    "    df.iloc[:, 1:5] = df.iloc[:, 1:5].apply(pd.to_numeric, errors='coerce')\n",
    "    averages = df.iloc[:,1:].groupby('week_start_date').mean().reset_index()\n",
    "    sums = df.groupby(['station_name', 'week_start_date'])['pcpn'].apply(\n",
    "        lambda x: np.nan if x.isnull().all() else x.sum()).reset_index().drop(\n",
    "        'station_name', axis=1).groupby('week_start_date').mean().reset_index()\n",
    "\n",
    "    # Combine data\n",
    "    df_site = averages.copy()\n",
    "    df_site['pcpn_sum'] = sums['pcpn']\n",
    "    df_site['site_id'] = gdf_sites.iloc[i]['site_id']\n",
    "    df_site = df_site[['site_id', 'week_start_date', 'pcpn_sum', 'pcpn_d', 'avgt', 'avgt_d']]\n",
    "\n",
    "    # Append row to data\n",
    "    df_all.append(df_site)\n",
    "\n",
    "# Export dataframe\n",
    "result = pd.concat(df_all)\n",
    "result.to_csv('assets/data/acis/acis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e7118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
