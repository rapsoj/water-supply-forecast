{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0e7a74",
   "metadata": {},
   "source": [
    "# Process NRCS Additional Sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147742e",
   "metadata": {},
   "source": [
    "### Prepare Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "023a4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import system libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import data manipulation librariaes\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import geospatial libraries\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "\n",
    "# Import API libraries\n",
    "import requests\n",
    "\n",
    "# Import progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/Users/jessicarapson/Documents/GitHub/water-supply-forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f708d",
   "metadata": {},
   "source": [
    "### Load Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a3c63217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original sites\n",
    "mflow_orig = pd.read_csv('assets/data/train_monthly_naturalized_flow.csv')\n",
    "metad_orig = pd.read_csv('assets/data/metadata.csv')\n",
    "\n",
    "# Create dictionary to change original site ids\n",
    "id_dict = metad_orig.set_index('site_id')['nrcs_id'].to_dict()\n",
    "\n",
    "# Load additional sites\n",
    "mflow_ncrs = pd.read_csv('assets/data/supplementary_nrcs_train_monthly_naturalized_flow.csv')\n",
    "metad_ncrs = pd.read_csv('assets/data/supplementary_nrcs_metadata.csv')\n",
    "\n",
    "# Transform data for combination\n",
    "mflow_orig = mflow_orig.rename(columns={'site_id': 'nrcs_id'})\n",
    "mflow_orig['nrcs_id'] = mflow_orig['nrcs_id'].replace(id_dict)\n",
    "metad_ncrs['season_start_month'] = 4\n",
    "metad_ncrs['season_end_month'] = 7\n",
    "metad_orig = metad_orig[metad_ncrs.columns]\n",
    "\n",
    "# Combine dataframes\n",
    "all_mflow = pd.concat([mflow_orig, mflow_ncrs], ignore_index=True)\n",
    "all_metad = pd.concat([metad_orig, metad_ncrs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23528929",
   "metadata": {},
   "source": [
    "### Add HydroBASIN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b431e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in attribute geospatial data (this takes a while)\n",
    "gdf_basins = gpd.read_file('assets/data/additional_sites/hydroBASINS_additional.gpkg')\n",
    "\n",
    "# Convert coordinates to geometry\n",
    "geometry = [Point(xy) for xy in zip(all_metad['longitude'], all_metad['latitude'])]\n",
    "gdf_sites = gpd.GeoDataFrame(all_metad, geometry=geometry, crs=gdf_basins.crs)\n",
    "\n",
    "# Select columns of interest\n",
    "cols_int = ['inu_pc_smn','inu_pc_smx','inu_pc_slt','inu_pc_umn','inu_pc_umx',\n",
    "            'inu_pc_ult','lka_pc_sse','lka_pc_use','dor_pc_pva','slp_dg_sav',\n",
    "            'slp_dg_uav','sgr_dk_sav','tmp_dc_uyr','ari_ix_sav','ari_ix_uav',\n",
    "            'cmi_ix_uyr','snw_pc_uyr','glc_pc_s01','glc_pc_s02','glc_pc_s03',\n",
    "            'glc_pc_s04','glc_pc_s05','glc_pc_s06','glc_pc_s07','glc_pc_s08',\n",
    "            'glc_pc_s09','glc_pc_s10','glc_pc_s11','glc_pc_s12','glc_pc_s13',\n",
    "            'glc_pc_s14','glc_pc_s15','glc_pc_s16','glc_pc_s17','glc_pc_s18',\n",
    "            'glc_pc_s19','glc_pc_s20','glc_pc_s21','glc_pc_s22','glc_pc_u01',\n",
    "            'glc_pc_u02','glc_pc_u03','glc_pc_u04','glc_pc_u05','glc_pc_u06',\n",
    "            'glc_pc_u07','glc_pc_u08','glc_pc_u09','glc_pc_u10','glc_pc_u11',\n",
    "            'glc_pc_u12','glc_pc_u13','glc_pc_u14','glc_pc_u15','glc_pc_u16',\n",
    "            'glc_pc_u17','glc_pc_u18','glc_pc_u19','glc_pc_u20','glc_pc_u21',\n",
    "            'glc_pc_u22','wet_pc_sg1','wet_pc_sg2','wet_pc_ug1','wet_pc_ug2',\n",
    "            'for_pc_sse','for_pc_use','crp_pc_sse','crp_pc_use','pst_pc_sse',\n",
    "            'pst_pc_use','ire_pc_sse','ire_pc_use','gla_pc_sse','gla_pc_use',\n",
    "            'prm_pc_sse','prm_pc_use','pac_pc_sse','pac_pc_use','cly_pc_sav',\n",
    "            'cly_pc_uav','slt_pc_sav','slt_pc_uav','snd_pc_sav','snd_pc_uav',\n",
    "            'soc_th_sav','soc_th_uav','swc_pc_syr','swc_pc_uyr','swc_pc_s01',\n",
    "            'swc_pc_s02','swc_pc_s03','swc_pc_s04','swc_pc_s05','swc_pc_s06',\n",
    "            'swc_pc_s07','swc_pc_s08','swc_pc_s09','swc_pc_s10','swc_pc_s11',\n",
    "            'swc_pc_s12','kar_pc_sse','kar_pc_use','ero_kh_sav','ero_kh_uav',\n",
    "            'ppd_pk_sav','ppd_pk_uav','urb_pc_sse','urb_pc_use','nli_ix_sav',\n",
    "            'nli_ix_uav','rdd_mk_sav','rdd_mk_uav','hft_ix_s93','hft_ix_u93',\n",
    "            'hft_ix_s09','hft_ix_u09','gwt_cm_sav','run_mm_syr','lkv_mc_usu',\n",
    "            'rev_mc_usu','ria_ha_ssu','ria_ha_usu','riv_tc_ssu','riv_tc_usu',\n",
    "            'pre_mm_uyr','pet_mm_syr','pet_mm_s01', 'pet_mm_s02','pet_mm_s03',\n",
    "            'pet_mm_s04','pet_mm_s05','pet_mm_s06','pet_mm_s07','pet_mm_s08',\n",
    "            'pet_mm_s09','pet_mm_s10','pet_mm_s11','pet_mm_s12','pet_mm_uyr',\n",
    "            'aet_mm_syr','aet_mm_s01','aet_mm_s02','aet_mm_s03','aet_mm_s04',\n",
    "            'aet_mm_s05','aet_mm_s06','aet_mm_s07','aet_mm_s08','aet_mm_s09',\n",
    "            'aet_mm_s10','aet_mm_s11','aet_mm_s12','aet_mm_uyr','pop_ct_ssu',\n",
    "            'pop_ct_usu','clz_cl_smj','cls_cl_smj','glc_cl_smj','pnv_cl_smj',\n",
    "            'wet_cl_smj','tbi_cl_smj','tec_cl_smj','fmh_cl_smj','fec_cl_smj',\n",
    "            'lit_cl_smj', 'SUB_AREA', 'UP_AREA','COAST','ORDER_','ENDO','geometry']\n",
    "\n",
    "# Join relevant basin data\n",
    "gdf_join = gpd.sjoin(gdf_sites, gdf_basins[cols_int], how='left', op='within')\n",
    "\n",
    "# Code missing values as NaN\n",
    "gdf_join = gdf_join.replace(-9999, np.nan)\n",
    "\n",
    "# Replace points with basin polygon\n",
    "polygons = gdf_join.merge(gdf_basins, how='left', left_on='index_right', right_index=True)\n",
    "gdf_join.geometry = polygons.geometry_y\n",
    "\n",
    "# Drop extra columns\n",
    "cols_drop = ['nrcs_name', 'usgs_id', 'elevation', 'latitude', 'longitude',\n",
    "             'season_start_month', 'season_end_month', 'geometry', 'index_right']\n",
    "gdf_join = gdf_join.drop(cols_drop, axis=1)\n",
    "\n",
    "# Convert categorical columns to strings\n",
    "cols_cat = ['COAST', 'ORDER_', 'ENDO', 'clz_cl_smj', 'cls_cl_smj',\n",
    "            'glc_cl_smj', 'pnv_cl_smj', 'wet_cl_smj', 'tbi_cl_smj',\n",
    "            'tec_cl_smj', 'fmh_cl_smj', 'fec_cl_smj', 'lit_cl_smj']\n",
    "gdf_join[cols_cat] = gdf_join[cols_cat].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf4ff1",
   "metadata": {},
   "source": [
    "### Add SWANN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1a97c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select years\n",
    "START_YEAR = 1981\n",
    "END_YEAR = 2023\n",
    "YEARS = [y for y in range(START_YEAR, END_YEAR)]\n",
    "\n",
    "# Select data\n",
    "ROOT = 'https://climate.arizona.edu/data/UA_SWE/'\n",
    "LOCATION = 'data/swann'\n",
    "\n",
    "# Download data\n",
    "os.makedirs(LOCATION, exist_ok=True)\n",
    "for yr in tqdm(YEARS):\n",
    "    fn = f'UA_SWE_Depth_WY{yr}.nc'\n",
    "    url = ROOT + '/' + fn\n",
    "    target = os.path.join(LOCATION, fn)\n",
    "    with open(target, 'wb') as f:\n",
    "        response = requests.get(url, verify=False)\n",
    "        f.write(response.content)\n",
    "\n",
    "swe_volumes = []\n",
    "for yr in tqdm(YEARS):\n",
    "    fn = os.path.join('data/swann', f'UA_SWE_Depth_WY{yr}.nc')\n",
    "    try:\n",
    "        ds = rioxarray.open_rasterio(fn, variable='SWE', mask_and_scale=True)\n",
    "    except IOError:\n",
    "        continue\n",
    "\n",
    "    # Perform an initial clip to reduce the size of the array\n",
    "    ds = ds.rio.write_crs(4326)\n",
    "    ds = ds.rio.reproject(\"EPSG:4326\")\n",
    "\n",
    "    ds['SWE'] /= 1000  # mm -> m\n",
    "    ds = ds.fillna(0)\n",
    "    ds = ds.rio.write_crs(4326)\n",
    "    ds_clipped_rgn = ds.rio.clip(gdf_join.geometry.values)\n",
    "\n",
    "    for i in range(0, len(gdf_join)):\n",
    "        try:\n",
    "            # Clip to current polygon\n",
    "            ds_clipped_poly = ds_clipped_rgn.rio.clip([gdf_join.iloc[i].geometry])\n",
    "\n",
    "            # Set -999 to nan then convert to dataframe\n",
    "            df = ds_clipped_poly.to_dataframe()\n",
    "            df = df.dropna()\n",
    "            df = df.reset_index()\n",
    "\n",
    "            # Group by time and average over grid cells\n",
    "            swe_vol = df.groupby(['time'])['SWE'].mean()\n",
    "\n",
    "            # Format dataframe\n",
    "            swe_vol = pd.DataFrame(swe_vol).reset_index()\n",
    "            swe_vol['nrcs_id'] = gdf_join.iloc[i].nrcs_id\n",
    "            swe_vol = swe_vol.set_index(['nrcs_id', 'time'])\n",
    "            swe_vol = swe_vol.rename({'SWE': 'SWE_depth_m'}, axis=1)\n",
    "            swe_volumes.append(swe_vol)\n",
    "        except Exception as e:\n",
    "            print(f\"No data found for: {gdf_join.iloc[i]['nrcs_id']}\")\n",
    "            continue\n",
    "\n",
    "# Concatenate dataframes and write output\n",
    "swe_volumes = pd.concat(swe_volumes, axis=0)\n",
    "swe_volumes.to_csv(\"assets/data/additional_sites/swann_swe.csv\")\n",
    "\n",
    "# Filter to years of interest\n",
    "swe_volumes_week = swe_volumes.copy()\n",
    "swe_volumes_week['date'] = pd.to_datetime(swe_volumes['time']).dt.date\n",
    "swe_volumes_week = swe_volumes_week[pd.to_datetime(\n",
    "    swe_volumes_week['date']) >= pd.Timestamp(\"1985-01-01\")]\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = datetime.date(1985, 1, 1)\n",
    "end_date = datetime.date(2024, 1, 1)\n",
    "\n",
    "# Initialize an empty list to store weeks\n",
    "week_list = []\n",
    "\n",
    "# Generate weeks between start_date and end_date\n",
    "current_date = start_date\n",
    "while current_date < end_date:\n",
    "    for day in [1, 8, 15, 22]:\n",
    "        week = current_date + datetime.timedelta(days=(day - current_date.weekday() - 1))\n",
    "        if week < end_date:\n",
    "            week_list.append(week.strftime('%Y-%m-%d'))\n",
    "    current_date += datetime.timedelta(days=7)\n",
    "    \n",
    "# Function to round down the day to the nearest value less than or equal to the day\n",
    "def round_day_down(date):\n",
    "    day = date.day\n",
    "    nearest_values = [1, 8, 15, 22]\n",
    "\n",
    "    # Find the nearest value less than or equal to the day\n",
    "    rounded_day = max(filter(lambda x: x <= day, nearest_values))\n",
    "    return date.replace(day=rounded_day)\n",
    "\n",
    " # Create a new column 'Rounded_Day_Column' based on 'Date_Column'\n",
    "swe_volumes_week['week_start_date'] = swe_volumes_week['date'].apply(round_day_down)\n",
    "swe_volumes_week = swe_volumes_week.drop(['date', 'time'], axis=1)\n",
    "\n",
    "# Aggregate by week\n",
    "swe_volumes_week = swe_volumes_week.groupby(\n",
    "    ['nrcs_id','week_start_date']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962fd60",
   "metadata": {},
   "source": [
    "### Combine Data and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "75103fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link area to snow volume data\n",
    "swe_volumes_area = swe_volumes_week.merge(\n",
    "    gdf_join[['nrcs_id','SUB_AREA']], how='left', on='nrcs_id')\n",
    "\n",
    "# Scale snow volume by area\n",
    "swe_volumes_area['SWE_depth_m_AREA_SCALED'] = swe_volumes_area[\n",
    "    'SWE_depth_m'] * swe_volumes_area['SUB_AREA']\n",
    "\n",
    "# Drop area column\n",
    "swe_volumes_area = swe_volumes_area.drop('SUB_AREA', axis=1)\n",
    "\n",
    "# Export data\n",
    "all_metad.to_csv(\"assets/data/additional_sites/metadata.csv\")\n",
    "swe_volumes_area.to_csv(\"assets/data/additional_sites/swann_swe.csv\")\n",
    "all_mflow.to_csv(\"assets/data/additional_sites/train_monthly_naturalized_flow.csv\")\n",
    "gdf_join.to_csv(\"assets/data/additional_sites/hydrobasins_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1382b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
