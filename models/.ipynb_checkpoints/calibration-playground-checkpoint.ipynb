{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab3c6fe",
   "metadata": {},
   "source": [
    "# Calibration Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f6109",
   "metadata": {},
   "source": [
    "### Prepare Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35ba0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import system libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import data manipulation librariaes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import statistics libraries\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/Users/jessicarapson/Documents/GitHub/water-supply-forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28722051",
   "metadata": {},
   "source": [
    "### Perform Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "65ec3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import predictions\n",
    "final_val = pd.read_csv('models/calibration_data/final_val.csv')\n",
    "val_gt = pd.read_csv('models/calibration_data/val_gt.csv')\n",
    "\n",
    "# Assuming you have predictions for the 10th, 50th, and 90th percentiles separately\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "predictions_10th = final_val['volume_10']\n",
    "predictions_50th = final_val['volume_50']\n",
    "predictions_90th = final_val['volume_90']\n",
    "ground_truth = val_gt['volume']\n",
    "\n",
    "# Fit isotonic regression separately for each quantile\n",
    "iso_reg_10th = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_reg_50th = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_reg_90th = IsotonicRegression(out_of_bounds='clip')\n",
    "\n",
    "iso_reg_10th.fit(predictions_10th, ground_truth)\n",
    "iso_reg_50th.fit(predictions_50th, ground_truth)\n",
    "iso_reg_90th.fit(predictions_90th, ground_truth)\n",
    "\n",
    "# Calibrate predictions for each quantile separately\n",
    "calibrated_predictions_10th = iso_reg_10th.predict(predictions_10th)\n",
    "calibrated_predictions_50th = iso_reg_50th.predict(predictions_50th)\n",
    "calibrated_predictions_90th = iso_reg_90th.predict(predictions_90th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a4f19",
   "metadata": {},
   "source": [
    "### Calculate Pinball Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "25d63091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Quantile Loss: 109.10394376162246\n",
      "Average Mean Quantile Loss: 162.84570497572648\n"
     ]
    }
   ],
   "source": [
    "# Define a function to compute quantile loss for a single quantile\n",
    "def quantile_loss(y_true, y_pred, q):\n",
    "    residual = y_true - y_pred\n",
    "    return np.mean(2 * np.maximum(q * residual, (q - 1) * residual))\n",
    "\n",
    "# Calculate average mean quantile loss across quantiles of interest\n",
    "average_mean_quantile_loss = np.mean([\n",
    "    quantile_loss(ground_truth, predictions_10th, quantiles[0]),\n",
    "    quantile_loss(ground_truth, predictions_50th, quantiles[1]),\n",
    "    quantile_loss(ground_truth, predictions_90th, quantiles[2])\n",
    "])\n",
    "\n",
    "print(\"Average Mean Quantile Loss:\", average_mean_quantile_loss)\n",
    "\n",
    "# Calculate average mean quantile loss across quantiles of interest\n",
    "average_mean_quantile_loss = np.mean([\n",
    "    quantile_loss(ground_truth, calibrated_predictions_10th, quantiles[0]),\n",
    "    quantile_loss(ground_truth, calibrated_predictions_50th, quantiles[1]),\n",
    "    quantile_loss(ground_truth, calibrated_predictions_90th, quantiles[2])\n",
    "])\n",
    "\n",
    "print(\"Average Mean Quantile Loss:\", average_mean_quantile_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7668db2",
   "metadata": {},
   "source": [
    "### Perform Isotonic Regression With Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "becb0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "final_val = pd.read_csv('models/calibration_data/final_val.csv')\n",
    "val_gt = pd.read_csv('models/calibration_data/val_gt.csv')\n",
    "\n",
    "# List of quantiles\n",
    "quantiles = [10, 50, 90]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    final_val, val_gt, test_size=0.2, random_state=42)\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Dictionary to store calibrated predictions for each quantile across folds\n",
    "calibrated_predictions_mean = {}\n",
    "\n",
    "for quantile in quantiles:\n",
    "    col_name = f'volume_{quantile}'\n",
    "    \n",
    "    # Fit isotonic regression on training data for each quantile\n",
    "    iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso_reg.fit(X_train[col_name], y_train['volume'])\n",
    "    \n",
    "    # Initialize an empty list to store predictions for each fold\n",
    "    predictions_per_fold = []\n",
    "    \n",
    "    # Perform manual 5-fold cross-validation and predict on the test set\n",
    "    fold_size = len(X_test) // num_folds\n",
    "    remainder = len(X_test) % num_folds\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        end = start + fold_size + (1 if i < remainder else 0)\n",
    "        \n",
    "        # Predict on the test subset for this fold\n",
    "        fold_predictions = iso_reg.predict(X_test.iloc[start:end][[col_name]])\n",
    "        predictions_per_fold.append(fold_predictions)\n",
    "        start = end\n",
    "    \n",
    "    # Pad predictions of shorter folds with the mean of others to make them uniform length\n",
    "    max_len = max(len(pred) for pred in predictions_per_fold)\n",
    "    for i in range(len(predictions_per_fold)):\n",
    "        if len(predictions_per_fold[i]) < max_len:\n",
    "            diff = max_len - len(predictions_per_fold[i])\n",
    "            mean_to_pad = np.mean(predictions_per_fold[:i] + predictions_per_fold[i+1:], axis=0)[-diff:]\n",
    "            predictions_per_fold[i] = np.concatenate((predictions_per_fold[i], mean_to_pad))\n",
    "    \n",
    "    # Store the predictions from each fold in the dictionary\n",
    "    calibrated_predictions_mean[quantile] = np.mean(predictions_per_fold, axis=0)\n",
    "\n",
    "# Create a DataFrame with the mean of 5 predictions for each quantile across 5 folds\n",
    "X_test_mean = pd.DataFrame(calibrated_predictions_mean, columns=quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d5804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b53b5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y_min for 10th quantile: -196.2016\n",
      "Minimum Quantile Loss for 10th quantile: 184.24769699486214\n",
      "Best y_min for 50th quantile: 32.552246\n",
      "Minimum Quantile Loss for 50th quantile: 165.17173469323774\n",
      "Best y_min for 90th quantile: 128.88217098989898\n",
      "Minimum Quantile Loss for 90th quantile: 163.0222207465406\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "final_val = pd.read_csv('models/calibration_data/final_val.csv')\n",
    "val_gt = pd.read_csv('models/calibration_data/val_gt.csv')\n",
    "\n",
    "# List of quantiles\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Assuming you have predictions for 10th, 50th, and 90th quantiles\n",
    "predictions_10th = final_val['volume_10']\n",
    "predictions_50th = final_val['volume_50']\n",
    "predictions_90th = final_val['volume_90']\n",
    "\n",
    "# Assuming you have ground truth values\n",
    "ground_truth = val_gt['volume']\n",
    "\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Split data into training and validation sets for each quantile\n",
    "X_train_10, X_val_10, y_train, y_val = train_test_split(predictions_10th, ground_truth, test_size=0.2, random_state=42)\n",
    "X_train_50, X_val_50, _, _ = train_test_split(predictions_50th, ground_truth, test_size=0.2, random_state=42)\n",
    "X_train_90, X_val_90, _, _ = train_test_split(predictions_90th, ground_truth, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize variables to store best y_min and minimum quantile loss for each quantile\n",
    "best_y_min = {}\n",
    "min_quantile_loss = {}\n",
    "\n",
    "# Hyperparameter tuning for y_min for each quantile\n",
    "for idx, X_train, quantile in zip([0, 1, 2], [X_train_10, X_train_50, X_train_90], quantiles):\n",
    "    best_y_min[quantile] = None\n",
    "    min_quantile_loss[quantile] = float('inf')\n",
    "    \n",
    "    for y_min_candidate in np.linspace(np.min(X_train), np.max(X_train), num=100):\n",
    "        # Fit isotonic regression model with current y_min candidate\n",
    "        isotonic_model = IsotonicRegression(y_min=y_min_candidate)\n",
    "        isotonic_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Select the appropriate validation set for the current quantile\n",
    "        if idx == 0:\n",
    "            X_val = X_val_10\n",
    "        elif idx == 1:\n",
    "            X_val = X_val_50\n",
    "        else:\n",
    "            X_val = X_val_90\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        calibrated_predictions = isotonic_model.transform(X_val)\n",
    "        \n",
    "        # Calculate quantile loss for the current quantile\n",
    "        average_mean_quantile_loss = quantile_loss(y_val, calibrated_predictions, quantile)\n",
    "        \n",
    "        # Update best y_min and minimum quantile loss for the current quantile\n",
    "        if average_mean_quantile_loss < min_quantile_loss[quantile]:\n",
    "            min_quantile_loss[quantile] = average_mean_quantile_loss\n",
    "            best_y_min[quantile] = y_min_candidate\n",
    "\n",
    "print(\"Best y_min for 10th quantile:\", best_y_min[0.1])\n",
    "print(\"Minimum Quantile Loss for 10th quantile:\", min_quantile_loss[0.1])\n",
    "print(\"Best y_min for 50th quantile:\", best_y_min[0.5])\n",
    "print(\"Minimum Quantile Loss for 50th quantile:\", min_quantile_loss[0.5])\n",
    "print(\"Best y_min for 90th quantile:\", best_y_min[0.9])\n",
    "print(\"Minimum Quantile Loss for 90th quantile:\", min_quantile_loss[0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e45e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dfa23983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Quantile Loss: -10363.269924041899\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (145,) (3620,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[213], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Mean Quantile Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_mean_quantile_loss)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate average mean quantile loss across quantiles of interest\u001b[39;00m\n\u001b[1;32m     16\u001b[0m average_mean_quantile_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mquantile_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_mean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalibrated_predictions_10th\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     18\u001b[0m     quantile_loss(X_test_mean[\u001b[38;5;241m50\u001b[39m], calibrated_predictions_50th, quantiles[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m     19\u001b[0m     quantile_loss(X_test_mean[\u001b[38;5;241m90\u001b[39m], calibrated_predictions_90th, quantiles[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     20\u001b[0m ])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Mean Quantile Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_mean_quantile_loss)\n",
      "Cell \u001b[0;32mIn[213], line 3\u001b[0m, in \u001b[0;36mquantile_loss\u001b[0;34m(y_true, y_pred, q)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantile_loss\u001b[39m(y_true, y_pred, q):\n\u001b[0;32m----> 3\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(q \u001b[38;5;241m*\u001b[39m residual, (q \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m residual))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:5815\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   5814\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 5815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:1381\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1381\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:285\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    281\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:220\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(expressions\u001b[38;5;241m.\u001b[39mevaluate, op)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    223\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    224\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/computation/expressions.py:131\u001b[0m, in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    128\u001b[0m     _store_test_result(result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TEST_MODE:\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (145,) (3620,) "
     ]
    }
   ],
   "source": [
    "# Define a function to compute quantile loss for a single quantile\n",
    "def quantile_loss(y_true, y_pred, q):\n",
    "    residual = y_true - y_pred\n",
    "    return np.mean(2 * np.maximum(q * residual, (q - 1) * residual))\n",
    "\n",
    "# Calculate average mean quantile loss across quantiles of interest\n",
    "average_mean_quantile_loss = np.mean([\n",
    "    quantile_loss(ground_truth, predictions_10th, quantiles[0]),\n",
    "    quantile_loss(ground_truth, predictions_50th, quantiles[1]),\n",
    "    quantile_loss(ground_truth, predictions_90th, quantiles[2])\n",
    "])\n",
    "\n",
    "print(\"Average Mean Quantile Loss:\", average_mean_quantile_loss)\n",
    "\n",
    "# Calculate average mean quantile loss across quantiles of interest\n",
    "average_mean_quantile_loss = np.mean([\n",
    "    quantile_loss(X_test_mean[10], calibrated_predictions_10th, quantiles[0]),\n",
    "    quantile_loss(X_test_mean[50], calibrated_predictions_50th, quantiles[1]),\n",
    "    quantile_loss(X_test_mean[90], calibrated_predictions_90th, quantiles[2])\n",
    "])\n",
    "\n",
    "print(\"Average Mean Quantile Loss:\", average_mean_quantile_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ea5dc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "final_val = pd.read_csv('models/calibration_data/final_val.csv')\n",
    "val_gt = pd.read_csv('models/calibration_data/val_gt.csv')\n",
    "\n",
    "# List of quantiles\n",
    "quantiles = [10, 50, 90]\n",
    "\n",
    "# Initialize arrays to store fold-wise predictions for each quantile\n",
    "fold_wise_predictions = {quantile: np.full((len(val_gt), 5), np.nan) for quantile in quantiles}\n",
    "\n",
    "# Define number of splits for k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over each fold and perform isotonic regression for each quantile\n",
    "for i, (train_idx, val_idx) in enumerate(kfold.split(final_val)):\n",
    "    X_tr, X_val = final_val.iloc[train_idx], final_val.iloc[val_idx]\n",
    "    y_tr, y_val = val_gt.iloc[train_idx], val_gt.iloc[val_idx]\n",
    "\n",
    "    for quantile in quantiles:\n",
    "        col_name = f'volume_{quantile}'\n",
    "\n",
    "        # Instantiate an Isotonic Regression model\n",
    "        iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "\n",
    "        # Fit the isotonic regression model\n",
    "        iso_reg.fit(X_tr[col_name], y_tr['volume'])\n",
    "\n",
    "        # Make predictions on the entire dataset\n",
    "        fold_preds = iso_reg.predict(final_val[col_name])\n",
    "\n",
    "        # Store fold-wise predictions for each quantile only in validation split\n",
    "        fold_wise_predictions[quantile][val_idx, i] = fold_preds[val_idx]\n",
    "\n",
    "# Calculate the average of predictions from each fold for each quantile\n",
    "final_predictions = {quantile: np.nanmean(fold_wise_predictions[quantile], axis=1) for quantile in quantiles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0e234858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Quantile Loss: 109.10394376162246\n",
      "Average Mean Quantile Loss: 169.62822662551142\n"
     ]
    }
   ],
   "source": [
    "# Define a function to compute quantile loss for a single quantile\n",
    "def quantile_loss(y_true, y_pred, q):\n",
    "    residual = y_true - y_pred\n",
    "    return np.mean(2 * np.maximum(q * residual, (q - 1) * residual))\n",
    "\n",
    "# Calculate average mean quantile loss across quantiles of interest\n",
    "average_mean_quantile_loss = np.mean([\n",
    "    quantile_loss(ground_truth, final_val['volume_10'], quantiles[0] / 100),\n",
    "    quantile_loss(ground_truth, final_val['volume_50'], quantiles[1] / 100),\n",
    "    quantile_loss(ground_truth, final_val['volume_90'], quantiles[2] / 100)\n",
    "])\n",
    "\n",
    "print(\"Average Mean Quantile Loss:\", average_mean_quantile_loss)\n",
    "\n",
    "# Calculate average mean quantile loss across quantiles of interest\n",
    "average_mean_quantile_loss = np.mean([\n",
    "    quantile_loss(ground_truth, pd.Series(final_predictions[10]), quantiles[0] / 100),\n",
    "    quantile_loss(ground_truth, pd.Series(final_predictions[50]), quantiles[1] / 100),\n",
    "    quantile_loss(ground_truth, pd.Series(final_predictions[90]), quantiles[2] / 100)\n",
    "])\n",
    "\n",
    "print(\"Average Mean Quantile Loss:\", average_mean_quantile_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6ee102b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'final_val' contains predictions and 'val_gt' contains ground truth values\n",
    "# Load your data and ensure the columns are in the correct format\n",
    "\n",
    "# Extract predictions from the 'final_val' DataFrame\n",
    "predictions_10 = final_val['volume_10'].values\n",
    "predictions_50 = final_val['volume_50'].values\n",
    "predictions_90 = final_val['volume_90'].values\n",
    "\n",
    "# Extract ground truth values from the 'val_gt' DataFrame\n",
    "ground_truth_values = val_gt['volume'].values\n",
    "\n",
    "quantiles = [10, 50, 90]\n",
    "calibrated_predictions = []\n",
    "\n",
    "# Apply Platt scaling separately for each quantile\n",
    "for idx, quantile in enumerate(quantiles):\n",
    "    # Select predictions for the current quantile\n",
    "    if idx == 0:\n",
    "        predictions = predictions_10\n",
    "    elif idx == 1:\n",
    "        predictions = predictions_50\n",
    "    else:\n",
    "        predictions = predictions_90\n",
    "\n",
    "    # Sort the predictions and ground truth values\n",
    "    sorted_indices = np.argsort(predictions)\n",
    "    sorted_predictions = predictions[sorted_indices]\n",
    "    sorted_ground_truth = ground_truth_values[sorted_indices]\n",
    "\n",
    "    threshold_index = int(len(sorted_predictions) * quantile / 100)\n",
    "\n",
    "    # Treat as binary classification (above or below quantile threshold)\n",
    "    labels = np.where(np.arange(len(sorted_predictions)) < threshold_index, 0, 1)\n",
    "\n",
    "    # Fit a logistic regression model for Platt scaling\n",
    "    platt_model = LogisticRegression(solver='liblinear')\n",
    "    platt_model.fit(sorted_predictions.reshape(-1, 1), labels)\n",
    "\n",
    "    # Use the model to predict probabilities\n",
    "    calibrated_probs = platt_model.predict_proba(sorted_predictions.reshape(-1, 1))[:, 1]\n",
    "\n",
    "    # Calculate the transformed predictions based on calibrated probabilities\n",
    "    calibrated_prediction = np.interp(predictions, sorted_predictions, calibrated_probs)\n",
    "    calibrated_predictions.append(calibrated_prediction)\n",
    "\n",
    "# 'calibrated_predictions' now contains Platt scaled predictions for each quantile\n",
    "# You can use these calibrated predictions as needed\n",
    "\n",
    "# Assuming min_volume and max_volume are the minimum and maximum values in your original \"volume\" column\n",
    "min_volume = val_gt['volume'].min()\n",
    "max_volume = val_gt['volume'].max()\n",
    "\n",
    "# Rescale the calibrated predictions to the original volume scale\n",
    "rescaled_predictions = []\n",
    "for calibrated_prediction in calibrated_predictions:\n",
    "    # Map the calibrated predictions from [0, 1] back to the original volume scale\n",
    "    rescaled_prediction = calibrated_prediction * (max_volume - min_volume) + min_volume\n",
    "    rescaled_predictions.append(rescaled_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336fa8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
