{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from consts import JULY, DEF_QUANTILES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data loading:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "outputs": [],
   "source": [
    "with open('playground_input.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X, y = data['train']\n",
    "val_X, val_y = data['val']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fun stuff:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert np.all(self.X.forecast_year.iloc[:-1].values <= self.X.forecast_year.iloc[1:].values), \\\n",
    "            'Error - not sorted by forecast year!'\n",
    "\n",
    "        fy = self.X.forecast_year.iloc[idx]\n",
    "        init_ind = (self.X.forecast_year == fy).argmax()\n",
    "        # Create sequence from rows 0 to idx\n",
    "        sequence = self.X.iloc[init_ind:idx + 1].drop(columns='forecast_year').values\n",
    "        label = self.y.iloc[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=3, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        out_packed, _ = self.lstm(x_packed)\n",
    "        out_padded, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        # Apply the linear layer to the unpacked outputs\n",
    "        out = self.fc(out_padded)\n",
    "        return out[:, -1, :]  # Return the outputs for the last time step\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    # Sort the batch by sequence length in descending order\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sequences, labels = zip(*batch)\n",
    "    # Pad the sequences and stack the labels\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_sequences, labels, lengths\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "outputs": [],
   "source": [
    "def features2seqs(X: pd.DataFrame, y: pd.Series, train: bool = True):\n",
    "    X = X[X.date.dt.month <= JULY].drop(columns=['date'])\n",
    "    if train:\n",
    "        return SequenceDataset(X.iloc[:32], y.iloc[:32])\n",
    "\n",
    "    raise NotImplementedError"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "outputs": [],
   "source": [
    "bs = 32\n",
    "lr = 1e-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "outputs": [],
   "source": [
    "def quantile_loss(quantile: float):\n",
    "    def qloss(y_true, y_pred):\n",
    "        return torch.mean(torch.max(quantile * (y_true - y_pred), -(1 - quantile) * (y_true - y_pred)))\n",
    "\n",
    "    return qloss\n",
    "\n",
    "\n",
    "def avg_quantile_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.stack([quantile_loss(q)(y_true, y_pred) for q in DEF_QUANTILES]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "outputs": [],
   "source": [
    "train_set = features2seqs(X, y)  # todo see we can overfit to a small training set before continuing\n",
    "combined_X = pd.concat([X, val_X])\n",
    "combined_y = pd.concat([y, val_y])\n",
    "combined_set = features2seqs(combined_X, combined_y)\n",
    "\n",
    "dataloader = DataLoader(train_set, batch_size=bs, shuffle=True, collate_fn=pad_collate_fn)\n",
    "\n",
    "n_feats = train_set[0][0].shape[1]\n",
    "model = LSTMModel(input_size=n_feats)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()  # todo implement AQM loss, requires multioutput (use dummy std for starters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "outputs": [
    {
     "data": {
      "text/plain": "      oniANOM  oniTOTAL  max_height  min_height    mjo70E  SWE_volume_m3  \\\n12  -0.499490 -0.854725    0.953922   -1.469843 -1.190694       0.167086   \n13  -0.533906 -0.875589    0.953922   -1.469843  0.015006       0.348750   \n14  -0.568321 -0.896453    0.953922   -1.469843  2.314010       0.335406   \n15  -0.563026 -0.843134    0.953922   -1.469843  1.118528       0.284683   \n16  -0.555463 -0.766965    0.953922   -1.469843 -0.138261       0.374512   \n..        ...       ...         ...         ...       ...            ...   \n851 -0.115778  0.493264    0.953922   -1.469843 -1.252000      -0.452350   \n852 -0.031366  0.471705    0.953922   -1.469843 -0.015647      -0.474310   \n853  0.034288  0.454937    0.953922   -1.469843  0.015006      -0.477171   \n854  0.099942  0.438168    0.953922   -1.469843  0.341975      -0.477171   \n855  0.134357  0.403395    0.953922   -1.469843  0.658727      -0.477171   \n\n     percent_diff_over_1000    soi_sd  catchment_area  site_max_height_diff  \\\n12                 1.998899  0.037966       -0.353614               3.11618   \n13                 1.998899  0.037966       -0.353614               3.11618   \n14                 1.998899  0.037966       -0.353614               3.11618   \n15                 1.998899  0.863044       -0.353614               3.11618   \n16                 1.998899  0.863044       -0.353614               3.11618   \n..                      ...       ...             ...                   ...   \n851                1.998899 -0.787112       -0.353614               3.11618   \n852                1.998899 -0.787112       -0.353614               3.11618   \n853                1.998899 -0.787112       -0.353614               3.11618   \n854                1.998899 -0.787112       -0.353614               3.11618   \n855                1.998899  0.141100       -0.353614               3.11618   \n\n     ...  percent_over_2000  ninoNINO1+2   mjo100E  med_height  \\\n12   ...            0.11328    -0.738090 -0.236802    0.140599   \n13   ...            0.11328    -0.738090 -1.829944    0.140599   \n14   ...            0.11328    -0.738090 -0.398133    0.140599   \n15   ...            0.11328     0.261551  0.670683    0.140599   \n16   ...            0.11328     0.261551  1.477337    0.140599   \n..   ...                ...          ...       ...         ...   \n851  ...            0.11328    -0.788998 -0.156137    0.140599   \n852  ...            0.11328    -0.788998 -0.761127    0.140599   \n853  ...            0.11328    -0.788998 -1.144288    0.140599   \n854  ...            0.11328    -0.788998 -0.942624    0.140599   \n855  ...            0.11328    -1.177747 -1.295536    0.140599   \n\n     percent_over_1000  ninoNINO3  ninoANOM.2  ninoANOM.3  ninoNINO4      time  \n12           -0.359338  -1.399001   -0.355988   -0.581102  -0.416069 -1.678157  \n13           -0.359338  -1.399001   -0.355988   -0.581102  -0.416069 -1.611691  \n14           -0.359338  -1.399001   -0.355988   -0.581102  -0.416069 -1.545224  \n15           -0.359338  -1.076117   -0.624270   -0.730655  -0.973397 -1.478758  \n16           -0.359338  -1.076117   -0.624270   -0.730655  -0.973397 -1.383807  \n..                 ...        ...         ...         ...        ...       ...  \n851          -0.359338  -0.083841    0.180574   -0.293499   0.642856 -0.044987  \n852          -0.359338  -0.083841    0.180574   -0.293499   0.642856  0.040470  \n853          -0.359338  -0.083841    0.180574   -0.293499   0.642856  0.106936  \n854          -0.359338  -0.083841    0.180574   -0.293499   0.642856  0.173402  \n855          -0.359338  -0.249221    0.359428    0.143657   0.740388  0.239868  \n\n[504 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>oniANOM</th>\n      <th>oniTOTAL</th>\n      <th>max_height</th>\n      <th>min_height</th>\n      <th>mjo70E</th>\n      <th>SWE_volume_m3</th>\n      <th>percent_diff_over_1000</th>\n      <th>soi_sd</th>\n      <th>catchment_area</th>\n      <th>site_max_height_diff</th>\n      <th>...</th>\n      <th>percent_over_2000</th>\n      <th>ninoNINO1+2</th>\n      <th>mjo100E</th>\n      <th>med_height</th>\n      <th>percent_over_1000</th>\n      <th>ninoNINO3</th>\n      <th>ninoANOM.2</th>\n      <th>ninoANOM.3</th>\n      <th>ninoNINO4</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>-0.499490</td>\n      <td>-0.854725</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>-1.190694</td>\n      <td>0.167086</td>\n      <td>1.998899</td>\n      <td>0.037966</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-0.738090</td>\n      <td>-0.236802</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-1.399001</td>\n      <td>-0.355988</td>\n      <td>-0.581102</td>\n      <td>-0.416069</td>\n      <td>-1.678157</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-0.533906</td>\n      <td>-0.875589</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>0.015006</td>\n      <td>0.348750</td>\n      <td>1.998899</td>\n      <td>0.037966</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-0.738090</td>\n      <td>-1.829944</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-1.399001</td>\n      <td>-0.355988</td>\n      <td>-0.581102</td>\n      <td>-0.416069</td>\n      <td>-1.611691</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.568321</td>\n      <td>-0.896453</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>2.314010</td>\n      <td>0.335406</td>\n      <td>1.998899</td>\n      <td>0.037966</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-0.738090</td>\n      <td>-0.398133</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-1.399001</td>\n      <td>-0.355988</td>\n      <td>-0.581102</td>\n      <td>-0.416069</td>\n      <td>-1.545224</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.563026</td>\n      <td>-0.843134</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>1.118528</td>\n      <td>0.284683</td>\n      <td>1.998899</td>\n      <td>0.863044</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>0.261551</td>\n      <td>0.670683</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-1.076117</td>\n      <td>-0.624270</td>\n      <td>-0.730655</td>\n      <td>-0.973397</td>\n      <td>-1.478758</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-0.555463</td>\n      <td>-0.766965</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>-0.138261</td>\n      <td>0.374512</td>\n      <td>1.998899</td>\n      <td>0.863044</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>0.261551</td>\n      <td>1.477337</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-1.076117</td>\n      <td>-0.624270</td>\n      <td>-0.730655</td>\n      <td>-0.973397</td>\n      <td>-1.383807</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>851</th>\n      <td>-0.115778</td>\n      <td>0.493264</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>-1.252000</td>\n      <td>-0.452350</td>\n      <td>1.998899</td>\n      <td>-0.787112</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-0.788998</td>\n      <td>-0.156137</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-0.083841</td>\n      <td>0.180574</td>\n      <td>-0.293499</td>\n      <td>0.642856</td>\n      <td>-0.044987</td>\n    </tr>\n    <tr>\n      <th>852</th>\n      <td>-0.031366</td>\n      <td>0.471705</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>-0.015647</td>\n      <td>-0.474310</td>\n      <td>1.998899</td>\n      <td>-0.787112</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-0.788998</td>\n      <td>-0.761127</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-0.083841</td>\n      <td>0.180574</td>\n      <td>-0.293499</td>\n      <td>0.642856</td>\n      <td>0.040470</td>\n    </tr>\n    <tr>\n      <th>853</th>\n      <td>0.034288</td>\n      <td>0.454937</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>0.015006</td>\n      <td>-0.477171</td>\n      <td>1.998899</td>\n      <td>-0.787112</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-0.788998</td>\n      <td>-1.144288</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-0.083841</td>\n      <td>0.180574</td>\n      <td>-0.293499</td>\n      <td>0.642856</td>\n      <td>0.106936</td>\n    </tr>\n    <tr>\n      <th>854</th>\n      <td>0.099942</td>\n      <td>0.438168</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>0.341975</td>\n      <td>-0.477171</td>\n      <td>1.998899</td>\n      <td>-0.787112</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-0.788998</td>\n      <td>-0.942624</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-0.083841</td>\n      <td>0.180574</td>\n      <td>-0.293499</td>\n      <td>0.642856</td>\n      <td>0.173402</td>\n    </tr>\n    <tr>\n      <th>855</th>\n      <td>0.134357</td>\n      <td>0.403395</td>\n      <td>0.953922</td>\n      <td>-1.469843</td>\n      <td>0.658727</td>\n      <td>-0.477171</td>\n      <td>1.998899</td>\n      <td>0.141100</td>\n      <td>-0.353614</td>\n      <td>3.11618</td>\n      <td>...</td>\n      <td>0.11328</td>\n      <td>-1.177747</td>\n      <td>-1.295536</td>\n      <td>0.140599</td>\n      <td>-0.359338</td>\n      <td>-0.249221</td>\n      <td>0.359428</td>\n      <td>0.143657</td>\n      <td>0.740388</td>\n      <td>0.239868</td>\n    </tr>\n  </tbody>\n</table>\n<p>504 rows Ã— 38 columns</p>\n</div>"
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X.date.dt.month <= JULY].drop(columns=['date', 'forecast_year'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [
    {
     "data": {
      "text/plain": "0    -0.032552\n1    -0.032552\n2    -0.032552\n3    -0.032552\n4    -0.032552\n5    -0.032552\n6    -0.032552\n7    -0.032552\n8    -0.032552\n9    -0.032552\n10   -0.032552\n11   -0.032552\n12   -0.032552\n13   -0.032552\n14   -0.032552\n15   -0.032552\n16   -0.032552\n17   -0.032552\n18   -0.032552\n19   -0.032552\n20   -0.032552\n21   -0.032552\n22   -0.032552\n23   -0.032552\n24   -0.032552\n25   -0.032552\n26   -0.032552\n27   -0.032552\n28   -1.063455\n29   -1.063455\n30   -1.063455\n31   -1.063455\nName: volume, dtype: float64"
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:32]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Loss: 0.1461\n",
      "Epoch [2/250], Loss: 2.7061\n",
      "Epoch [3/250], Loss: 0.2445\n",
      "Epoch [4/250], Loss: 0.2809\n",
      "Epoch [5/250], Loss: 0.2125\n",
      "Epoch [6/250], Loss: 0.1738\n",
      "Epoch [7/250], Loss: 0.1435\n",
      "Epoch [8/250], Loss: 0.1266\n",
      "Epoch [9/250], Loss: 0.1231\n",
      "Epoch [10/250], Loss: 0.1285\n",
      "Epoch [11/250], Loss: 0.1363\n",
      "Epoch [12/250], Loss: 0.1416\n",
      "Epoch [13/250], Loss: 0.1422\n",
      "Epoch [14/250], Loss: 0.1387\n",
      "Epoch [15/250], Loss: 0.1334\n",
      "Epoch [16/250], Loss: 0.1287\n",
      "Epoch [17/250], Loss: 0.1259\n",
      "Epoch [18/250], Loss: 0.1249\n",
      "Epoch [19/250], Loss: 0.1248\n",
      "Epoch [20/250], Loss: 0.1245\n",
      "Epoch [21/250], Loss: 0.1237\n",
      "Epoch [22/250], Loss: 0.1224\n",
      "Epoch [23/250], Loss: 0.1209\n",
      "Epoch [24/250], Loss: 0.1199\n",
      "Epoch [25/250], Loss: 0.1198\n",
      "Epoch [26/250], Loss: 0.1204\n",
      "Epoch [27/250], Loss: 0.1212\n",
      "Epoch [28/250], Loss: 0.1215\n",
      "Epoch [29/250], Loss: 0.1211\n",
      "Epoch [30/250], Loss: 0.1199\n",
      "Epoch [31/250], Loss: 0.1183\n",
      "Epoch [32/250], Loss: 0.1169\n",
      "Epoch [33/250], Loss: 0.1162\n",
      "Epoch [34/250], Loss: 0.1164\n",
      "Epoch [35/250], Loss: 0.1171\n",
      "Epoch [36/250], Loss: 0.1180\n",
      "Epoch [37/250], Loss: 0.1185\n",
      "Epoch [38/250], Loss: 0.1185\n",
      "Epoch [39/250], Loss: 0.1178\n",
      "Epoch [40/250], Loss: 0.1169\n",
      "Epoch [41/250], Loss: 0.1161\n",
      "Epoch [42/250], Loss: 0.1157\n",
      "Epoch [43/250], Loss: 0.1158\n",
      "Epoch [44/250], Loss: 0.1162\n",
      "Epoch [45/250], Loss: 0.1167\n",
      "Epoch [46/250], Loss: 0.1169\n",
      "Epoch [47/250], Loss: 0.1168\n",
      "Epoch [48/250], Loss: 0.1165\n",
      "Epoch [49/250], Loss: 0.1161\n",
      "Epoch [50/250], Loss: 0.1159\n",
      "Epoch [51/250], Loss: 0.1158\n",
      "Epoch [52/250], Loss: 0.1159\n",
      "Epoch [53/250], Loss: 0.1160\n",
      "Epoch [54/250], Loss: 0.1161\n",
      "Epoch [55/250], Loss: 0.1161\n",
      "Epoch [56/250], Loss: 0.1161\n",
      "Epoch [57/250], Loss: 0.1159\n",
      "Epoch [58/250], Loss: 0.1159\n",
      "Epoch [59/250], Loss: 0.1158\n",
      "Epoch [60/250], Loss: 0.1158\n",
      "Epoch [61/250], Loss: 0.1158\n",
      "Epoch [62/250], Loss: 0.1158\n",
      "Epoch [63/250], Loss: 0.1158\n",
      "Epoch [64/250], Loss: 0.1158\n",
      "Epoch [65/250], Loss: 0.1158\n",
      "Epoch [66/250], Loss: 0.1158\n",
      "Epoch [67/250], Loss: 0.1158\n",
      "Epoch [68/250], Loss: 0.1158\n",
      "Epoch [69/250], Loss: 0.1158\n",
      "Epoch [70/250], Loss: 0.1157\n",
      "Epoch [71/250], Loss: 0.1157\n",
      "Epoch [72/250], Loss: 0.1157\n",
      "Epoch [73/250], Loss: 0.1157\n",
      "Epoch [74/250], Loss: 0.1158\n",
      "Epoch [75/250], Loss: 0.1158\n",
      "Epoch [76/250], Loss: 0.1158\n",
      "Epoch [77/250], Loss: 0.1157\n",
      "Epoch [78/250], Loss: 0.1157\n",
      "Epoch [79/250], Loss: 0.1157\n",
      "Epoch [80/250], Loss: 0.1157\n",
      "Epoch [81/250], Loss: 0.1157\n",
      "Epoch [82/250], Loss: 0.1157\n",
      "Epoch [83/250], Loss: 0.1157\n",
      "Epoch [84/250], Loss: 0.1157\n",
      "Epoch [85/250], Loss: 0.1157\n",
      "Epoch [86/250], Loss: 0.1157\n",
      "Epoch [87/250], Loss: 0.1157\n",
      "Epoch [88/250], Loss: 0.1157\n",
      "Epoch [89/250], Loss: 0.1157\n",
      "Epoch [90/250], Loss: 0.1157\n",
      "Epoch [91/250], Loss: 0.1157\n",
      "Epoch [92/250], Loss: 0.1157\n",
      "Epoch [93/250], Loss: 0.1157\n",
      "Epoch [94/250], Loss: 0.1157\n",
      "Epoch [95/250], Loss: 0.1157\n",
      "Epoch [96/250], Loss: 0.1157\n",
      "Epoch [97/250], Loss: 0.1157\n",
      "Epoch [98/250], Loss: 0.1157\n",
      "Epoch [99/250], Loss: 0.1157\n",
      "Epoch [100/250], Loss: 0.1157\n",
      "Epoch [101/250], Loss: 0.1157\n",
      "Epoch [102/250], Loss: 0.1157\n",
      "Epoch [103/250], Loss: 0.1157\n",
      "Epoch [104/250], Loss: 0.1157\n",
      "Epoch [105/250], Loss: 0.1157\n",
      "Epoch [106/250], Loss: 0.1157\n",
      "Epoch [107/250], Loss: 0.1157\n",
      "Epoch [108/250], Loss: 0.1157\n",
      "Epoch [109/250], Loss: 0.1157\n",
      "Epoch [110/250], Loss: 0.1157\n",
      "Epoch [111/250], Loss: 0.1157\n",
      "Epoch [112/250], Loss: 0.1157\n",
      "Epoch [113/250], Loss: 0.1157\n",
      "Epoch [114/250], Loss: 0.1157\n",
      "Epoch [115/250], Loss: 0.1157\n",
      "Epoch [116/250], Loss: 0.1157\n",
      "Epoch [117/250], Loss: 0.1157\n",
      "Epoch [118/250], Loss: 0.1157\n",
      "Epoch [119/250], Loss: 0.1157\n",
      "Epoch [120/250], Loss: 0.1157\n",
      "Epoch [121/250], Loss: 0.1157\n",
      "Epoch [122/250], Loss: 0.1157\n",
      "Epoch [123/250], Loss: 0.1157\n",
      "Epoch [124/250], Loss: 0.1157\n",
      "Epoch [125/250], Loss: 0.1157\n",
      "Epoch [126/250], Loss: 0.1157\n",
      "Epoch [127/250], Loss: 0.1157\n",
      "Epoch [128/250], Loss: 0.1157\n",
      "Epoch [129/250], Loss: 0.1157\n",
      "Epoch [130/250], Loss: 0.1157\n",
      "Epoch [131/250], Loss: 0.1157\n",
      "Epoch [132/250], Loss: 0.1157\n",
      "Epoch [133/250], Loss: 0.1157\n",
      "Epoch [134/250], Loss: 0.1157\n",
      "Epoch [135/250], Loss: 0.1157\n",
      "Epoch [136/250], Loss: 0.1157\n",
      "Epoch [137/250], Loss: 0.1157\n",
      "Epoch [138/250], Loss: 0.1157\n",
      "Epoch [139/250], Loss: 0.1157\n",
      "Epoch [140/250], Loss: 0.1157\n",
      "Epoch [141/250], Loss: 0.1157\n",
      "Epoch [142/250], Loss: 0.1157\n",
      "Epoch [143/250], Loss: 0.1157\n",
      "Epoch [144/250], Loss: 0.1157\n",
      "Epoch [145/250], Loss: 0.1157\n",
      "Epoch [146/250], Loss: 0.1157\n",
      "Epoch [147/250], Loss: 0.1157\n",
      "Epoch [148/250], Loss: 0.1157\n",
      "Epoch [149/250], Loss: 0.1157\n",
      "Epoch [150/250], Loss: 0.1157\n",
      "Epoch [151/250], Loss: 0.1157\n",
      "Epoch [152/250], Loss: 0.1157\n",
      "Epoch [153/250], Loss: 0.1157\n",
      "Epoch [154/250], Loss: 0.1157\n",
      "Epoch [155/250], Loss: 0.1157\n",
      "Epoch [156/250], Loss: 0.1157\n",
      "Epoch [157/250], Loss: 0.1157\n",
      "Epoch [158/250], Loss: 0.1157\n",
      "Epoch [159/250], Loss: 0.1157\n",
      "Epoch [160/250], Loss: 0.1157\n",
      "Epoch [161/250], Loss: 0.1157\n",
      "Epoch [162/250], Loss: 0.1157\n",
      "Epoch [163/250], Loss: 0.1157\n",
      "Epoch [164/250], Loss: 0.1157\n",
      "Epoch [165/250], Loss: 0.1157\n",
      "Epoch [166/250], Loss: 0.1157\n",
      "Epoch [167/250], Loss: 0.1157\n",
      "Epoch [168/250], Loss: 0.1157\n",
      "Epoch [169/250], Loss: 0.1157\n",
      "Epoch [170/250], Loss: 0.1157\n",
      "Epoch [171/250], Loss: 0.1157\n",
      "Epoch [172/250], Loss: 0.1157\n",
      "Epoch [173/250], Loss: 0.1157\n",
      "Epoch [174/250], Loss: 0.1157\n",
      "Epoch [175/250], Loss: 0.1157\n",
      "Epoch [176/250], Loss: 0.1157\n",
      "Epoch [177/250], Loss: 0.1157\n",
      "Epoch [178/250], Loss: 0.1157\n",
      "Epoch [179/250], Loss: 0.1157\n",
      "Epoch [180/250], Loss: 0.1157\n",
      "Epoch [181/250], Loss: 0.1157\n",
      "Epoch [182/250], Loss: 0.1157\n",
      "Epoch [183/250], Loss: 0.1157\n",
      "Epoch [184/250], Loss: 0.1157\n",
      "Epoch [185/250], Loss: 0.1157\n",
      "Epoch [186/250], Loss: 0.1157\n",
      "Epoch [187/250], Loss: 0.1157\n",
      "Epoch [188/250], Loss: 0.1157\n",
      "Epoch [189/250], Loss: 0.1157\n",
      "Epoch [190/250], Loss: 0.1157\n",
      "Epoch [191/250], Loss: 0.1157\n",
      "Epoch [192/250], Loss: 0.1157\n",
      "Epoch [193/250], Loss: 0.1157\n",
      "Epoch [194/250], Loss: 0.1157\n",
      "Epoch [195/250], Loss: 0.1157\n",
      "Epoch [196/250], Loss: 0.1157\n",
      "Epoch [197/250], Loss: 0.1157\n",
      "Epoch [198/250], Loss: 0.1157\n",
      "Epoch [199/250], Loss: 0.1157\n",
      "Epoch [200/250], Loss: 0.1157\n",
      "Epoch [201/250], Loss: 0.1157\n",
      "Epoch [202/250], Loss: 0.1157\n",
      "Epoch [203/250], Loss: 0.1157\n",
      "Epoch [204/250], Loss: 0.1157\n",
      "Epoch [205/250], Loss: 0.1157\n",
      "Epoch [206/250], Loss: 0.1157\n",
      "Epoch [207/250], Loss: 0.1157\n",
      "Epoch [208/250], Loss: 0.1157\n",
      "Epoch [209/250], Loss: 0.1157\n",
      "Epoch [210/250], Loss: 0.1157\n",
      "Epoch [211/250], Loss: 0.1157\n",
      "Epoch [212/250], Loss: 0.1157\n",
      "Epoch [213/250], Loss: 0.1157\n",
      "Epoch [214/250], Loss: 0.1157\n",
      "Epoch [215/250], Loss: 0.1157\n",
      "Epoch [216/250], Loss: 0.1157\n",
      "Epoch [217/250], Loss: 0.1157\n",
      "Epoch [218/250], Loss: 0.1157\n",
      "Epoch [219/250], Loss: 0.1157\n",
      "Epoch [220/250], Loss: 0.1157\n",
      "Epoch [221/250], Loss: 0.1157\n",
      "Epoch [222/250], Loss: 0.1157\n",
      "Epoch [223/250], Loss: 0.1157\n",
      "Epoch [224/250], Loss: 0.1157\n",
      "Epoch [225/250], Loss: 0.1157\n",
      "Epoch [226/250], Loss: 0.1157\n",
      "Epoch [227/250], Loss: 0.1157\n",
      "Epoch [228/250], Loss: 0.1157\n",
      "Epoch [229/250], Loss: 0.1157\n",
      "Epoch [230/250], Loss: 0.1157\n",
      "Epoch [231/250], Loss: 0.1157\n",
      "Epoch [232/250], Loss: 0.1157\n",
      "Epoch [233/250], Loss: 0.1157\n",
      "Epoch [234/250], Loss: 0.1157\n",
      "Epoch [235/250], Loss: 0.1157\n",
      "Epoch [236/250], Loss: 0.1157\n",
      "Epoch [237/250], Loss: 0.1157\n",
      "Epoch [238/250], Loss: 0.1157\n",
      "Epoch [239/250], Loss: 0.1157\n",
      "Epoch [240/250], Loss: 0.1157\n",
      "Epoch [241/250], Loss: 0.1157\n",
      "Epoch [242/250], Loss: 0.1157\n",
      "Epoch [243/250], Loss: 0.1157\n",
      "Epoch [244/250], Loss: 0.1157\n",
      "Epoch [245/250], Loss: 0.1157\n",
      "Epoch [246/250], Loss: 0.1157\n",
      "Epoch [247/250], Loss: 0.1157\n",
      "Epoch [248/250], Loss: 0.1157\n",
      "Epoch [249/250], Loss: 0.1157\n",
      "Epoch [250/250], Loss: 0.1157\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    for sequences, labels, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences, lengths)\n",
    "        outputs = outputs.squeeze()  # todo remove/change when using a multioutput\n",
    "        # Ensure labels are also squeezed to match output shape\n",
    "        loss = criterion(labels, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Empirical quantiles training loss:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "outputs": [
    {
     "data": {
      "text/plain": "0.2405846650599108"
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([mean_pinball_loss(y, [y.quantile(q)] * len(y), alpha=q) for q in DEF_QUANTILES])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
