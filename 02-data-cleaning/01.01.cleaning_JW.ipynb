{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in libraries\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# Set the directory path where your files are located\n",
    "folder_path = \"C:\\\\Users\\\\johnh\\\\OneDrive\\\\Documents\\\\GitHub\\\\water-supply-forecast\\\\assets\\\\data\\\\teleconnections\"\n",
    "folder_path_flow = \"C:\\\\Users\\\\johnh\\\\OneDrive\\\\Documents\\\\GitHub\\\\water-supply-forecast\\\\assets\\\\data\"\n",
    "folder_path_grace = \"C:\\\\Users\\\\johnh\\\\OneDrive\\\\Documents\\\\GitHub\\\\water-supply-forecast\\\\assets\\\\data\\\\grace_indicators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map month abbreviations to numeric values\n",
    "month_to_num = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,\n",
    "    'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8,\n",
    "    'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "# Dictionary to map month abbreviations to numeric values\n",
    "month_to_num_up = {\n",
    "    'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4,\n",
    "    'MAY': 5, 'JUN': 6, 'JUL': 7, 'AUG': 8,\n",
    "    'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for mjo dataset\n",
    "df_mjo = pd.read_table(os.path.join(folder_path,\"mjo.txt\"),delim_whitespace=True, skiprows=1)\n",
    "df_mjo = df_mjo.iloc[1:]\n",
    "df_mjo.columns = df_mjo.columns.str.strip()\n",
    "df_mjo = df_mjo.add_prefix('mjo')\n",
    "df_mjo = df_mjo[df_mjo['mjo20E'] != '*****'] # Remove future values (missing)\n",
    "\n",
    "df_mjo['year'] = df_mjo['mjoPENTAD'].astype(str).str[:4].astype(int)\n",
    "df_mjo['month'] = df_mjo['mjoPENTAD'].astype(str).str[4:6].astype(int)\n",
    "df_mjo['day'] = df_mjo['mjoPENTAD'].astype(str).str[6:8].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for nino dataset\n",
    "df_nino = pd.read_table(os.path.join(folder_path,\"nino_regions_sst.txt\"),delim_whitespace=True)\n",
    "df_nino = df_nino.rename(columns={'YR':'year', 'MON':'month'})\n",
    "df_nino = df_nino.rename(columns={c: 'nino'+c for c in df_nino.columns if c not in ['year', 'month']})\n",
    "df_nino['day'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for oni dataset\n",
    "df_oni = pd.read_table(os.path.join(folder_path,\"oni.txt\"),delim_whitespace=True)\n",
    "df_oni = df_oni.rename(columns={'YR':'year'})\n",
    "df_oni['month'] = 1 #Assume month of collection is january\n",
    "df_oni = df_oni.rename(columns={c: 'oni'+c for c in df_oni.columns if c not in ['year', 'month']})\n",
    "df_oni['day'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for pdo dataset\n",
    "df_pdo = pd.read_table(os.path.join(folder_path,\"pdo.txt\"),delim_whitespace=True,skiprows=1)\n",
    "df_pdo = pd.melt(df_pdo, id_vars=['Year'], var_name='Month', value_name='pdo')\n",
    "df_pdo = df_pdo.rename(columns={'Year':'year', 'Month':'month'})\n",
    "df_pdo = df_pdo[df_pdo['pdo'] != 99.99] # Remove future values (missing)\n",
    "df_pdo['month'] = df_pdo['month'].map(month_to_num)\n",
    "df_pdo['day'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for pna dataset\n",
    "df_pna = pd.read_table(os.path.join(folder_path,\"pna.txt\"),delim_whitespace=True)\n",
    "df_pna = pd.melt(df_pna, id_vars=['year'], var_name='month', value_name='pna')\n",
    "df_pna['month'] = df_pna['month'].map(month_to_num)\n",
    "df_pna['day']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for soi dataset\n",
    "df_soi1 = pd.read_table(os.path.join(folder_path,\"soi1.txt\"),delim_whitespace=True,skiprows=3)\n",
    "df_soi1.columns = df_soi1.columns.str.strip()\n",
    "df_soi1 = pd.melt(df_soi1, id_vars=['YEAR'], var_name='month', value_name='soi_anom')\n",
    "df_soi1 = df_soi1.rename(columns={'YEAR':'year'})\n",
    "df_soi1['month'] = df_soi1['month'].map(month_to_num_up)\n",
    "df_soi1['day']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for soi dataset\n",
    "df_soi2 = pd.read_table(os.path.join(folder_path,\"soi2.txt\"),delim_whitespace=True,skiprows=3)\n",
    "df_soi2.columns = df_soi2.columns.str.strip()\n",
    "df_soi2 = pd.melt(df_soi2, id_vars=['YEAR'], var_name='month', value_name='soi_sd')\n",
    "df_soi2 = df_soi2.rename(columns={'YEAR':'year'})\n",
    "df_soi2['month'] = df_soi2['month'].map(month_to_num_up)\n",
    "df_soi2['day']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge code\n",
    "data_frames = [df_mjo, df_nino, df_oni, df_pdo, df_pna, df_soi1, df_soi2]\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['year', 'month', 'day'],\n",
    "                                            how='outer'), data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in streamflows\n",
    "df_flow = pd.read_csv(os.path.join(folder_path_flow,\"train_monthly_naturalized_flow.csv\"))\n",
    "df_flow['day']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge code\n",
    "data_frames = [df_mjo, df_nino, df_oni, df_pdo, df_pna, df_soi1, df_soi2, df_flow]\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['year', 'month', 'day'],\n",
    "                                            how='outer'), data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for grace indicators\n",
    "df_grace = pd.read_csv(os.path.join(folder_path_grace,\"grace_aggregated.csv\"))\n",
    "\n",
    "# Convert 'time' to datetime format\n",
    "df_grace['time'] = pd.to_datetime(df_grace['time'])\n",
    "\n",
    "# Extract day, month, and year into separate columns\n",
    "df_grace['day'] = df_grace['time'].dt.day\n",
    "df_grace['month'] = df_grace['time'].dt.month\n",
    "df_grace['year'] = df_grace['time'].dt.year\n",
    "\n",
    "df_grace.drop('time', axis=1, inplace=True)\n",
    "\n",
    "data_frames = [df_merged, df_grace]\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['year', 'month', 'day', 'site_id'],\n",
    "                                            how='outer'), data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order columns\n",
    "column_order =  ['year'] + ['month'] + ['day'] + [col for col in df_merged.columns if col != ['day', 'month','year']]\n",
    "df_merged = df_merged[column_order]\n",
    "\n",
    "# Output the DataFrame to a CSV file\n",
    "df_merged.to_csv('merged_dataframe.csv', index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is adjusts variable types, then standardises numeric variables and one-hot encodes categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display variable names and data types\n",
    "variable_types_df = pd.DataFrame({'Variable': df_merged.columns, 'Data Type': df_merged.dtypes.values})\n",
    "\n",
    "# Iterate over columns\n",
    "for column_name in df_merged.columns:\n",
    "    # Check if the column name contains the substring 'mjo'\n",
    "    if 'mjo' in column_name:\n",
    "        # Convert values to float using pd.to_numeric\n",
    "        df_merged[column_name] = pd.to_numeric(df_merged[column_name], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "def preprocess_column(df, column_name):\n",
    "    # Skip preprocessing for specified columns\n",
    "    if column_name in [\"month\", \"year\", \"day\"]:\n",
    "        return df\n",
    "    \n",
    "    # Check the data type of the column\n",
    "    column_dtype = df[column_name].dtype\n",
    "    \n",
    "    if column_dtype == 'object':\n",
    "        # If it's a categorical variable, perform one-hot encoding\n",
    "        df = pd.get_dummies(df, columns=[column_name], prefix=[column_name])\n",
    "    elif column_dtype in ['int64', 'float64']:\n",
    "        # If it's a numeric variable, standardize it\n",
    "        scaler = StandardScaler()\n",
    "        df[column_name] = scaler.fit_transform(df[[column_name]])\n",
    "    elif column_dtype == 'bool':\n",
    "        # If it's a binary variable, check if it's 0-1 coded and encode if not\n",
    "        unique_values = df[column_name].unique()\n",
    "        if set(unique_values) == {0, 1}:\n",
    "            print(f\"{column_name} is already 0-1 coded.\")\n",
    "        else:\n",
    "            label_encoder = LabelEncoder()\n",
    "            df[column_name] = label_encoder.fit_transform(df[column_name])\n",
    "    else:\n",
    "        print(f\"Warning: Unsupported data type for column {column_name}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # Iterate over all columns in the DataFrame\n",
    "    for column_name in df.columns:\n",
    "        df = preprocess_column(df, column_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Perform preprocessing on all columns\n",
    "trans_vars = preprocess_dataframe(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the DataFrame to a CSV file\n",
    "trans_vars.to_csv('transformed_vars.csv', index=False)  # Set index=False to exclude the index column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
